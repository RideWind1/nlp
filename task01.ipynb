{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1.学习到了本地如何去配置一个nlp的环境，学习在本地使用notebook来编写代码，同时可以将本地的代码上传到colab，对于训练比较方便\n",
    "\n",
    "\n",
    "2.常见的NLP任务分为4大类，分别是1、文本分类， 2、序列标注，3、问答任务——抽取式问答和多选问答，4、生成任务——语言模型、机器翻译和摘要生成\n",
    "\n",
    "2017年，Attention Is All You Need论文首次提出了Transformer模型结构并在机器翻译任务上取得了The State of the Art(SOTA, 最好)的效果，transformer模型从此飞速发展，各种各样给予transformer的模型层出不穷，其中最有名的当属2018年，BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding使用Transformer模型结构进行大规模语言模型（language model）预训练（Pre-train），再在多个NLP下游（downstream）任务中进行微调（Finetune），一举刷新了各大NLP任务的榜单最高分，轰动一时。2019年-2021年，研究人员将Transformer这种模型结构和预训练+微调这种训练方式相结合，提出了一系列Transformer模型结构、训练方式的改进（比如transformer-xl，XLnet，Roberta等等），后面我们将进行tranformer的详细学习。"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}